{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7be04fb0ab09441d91761b7e8ae17ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c212ac9a2a5e45d39181fdcc574cf93c",
              "IPY_MODEL_9bb53239e3ca4a8e897d6b76bca41422",
              "IPY_MODEL_370c98d8187441d1becd120461928916"
            ],
            "layout": "IPY_MODEL_07edbcc4528e49ee93a987c6b292f48b"
          }
        },
        "c212ac9a2a5e45d39181fdcc574cf93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c32c2047e6b4383b2e78eeccd371083",
            "placeholder": "​",
            "style": "IPY_MODEL_502d3b3abe884483aca9d741f212340d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9bb53239e3ca4a8e897d6b76bca41422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2af81bb70824fdeb76d2fec7722b11e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce7ba90af64941f59a30da85e688ca62",
            "value": 4
          }
        },
        "370c98d8187441d1becd120461928916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44834d77d454a698548f0f50a3db61e",
            "placeholder": "​",
            "style": "IPY_MODEL_d963206dd9dd46fca40827c8563916d2",
            "value": " 4/4 [00:09&lt;00:00,  2.17s/it]"
          }
        },
        "07edbcc4528e49ee93a987c6b292f48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c32c2047e6b4383b2e78eeccd371083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "502d3b3abe884483aca9d741f212340d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2af81bb70824fdeb76d2fec7722b11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce7ba90af64941f59a30da85e688ca62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f44834d77d454a698548f0f50a3db61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d963206dd9dd46fca40827c8563916d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eb072ef79b643358a6cd955334f54d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89d73c283ea94613a772ced549d84d61",
              "IPY_MODEL_ecee4d34afc146fea34b3fb02b3c8d14",
              "IPY_MODEL_71a865a4511845e69b43e66c4e5d397a"
            ],
            "layout": "IPY_MODEL_1ebc944883f241578e7351cb56da3647"
          }
        },
        "89d73c283ea94613a772ced549d84d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b1a4a1b0e140399e7b0a50e379f28c",
            "placeholder": "​",
            "style": "IPY_MODEL_50cbf87a80db49b59191abc897e08267",
            "value": "Evaluating: 100%"
          }
        },
        "ecee4d34afc146fea34b3fb02b3c8d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e43393c407a48798eadf4eaaec2d8d2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bbeaa58dfee4cb8a8802dbf7df11360",
            "value": 18
          }
        },
        "71a865a4511845e69b43e66c4e5d397a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68d771e3d320466a9ab19f0f66bb9db7",
            "placeholder": "​",
            "style": "IPY_MODEL_1623bef71f8b4e4abd5a96a785f217ba",
            "value": " 18/18 [00:10&lt;00:00,  1.71it/s]"
          }
        },
        "1ebc944883f241578e7351cb56da3647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8b1a4a1b0e140399e7b0a50e379f28c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50cbf87a80db49b59191abc897e08267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e43393c407a48798eadf4eaaec2d8d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bbeaa58dfee4cb8a8802dbf7df11360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68d771e3d320466a9ab19f0f66bb9db7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1623bef71f8b4e4abd5a96a785f217ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PROYECTO FINAL - NLP\n",
        "\n",
        "Christian Hernández"
      ],
      "metadata": {
        "id": "Op1sluJVTDIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep"
      ],
      "metadata": {
        "id": "YMQV--U1zsLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate einops ragas langchain xformers bitsandbytes sentence_transformers chromadb langchain_community pypdf \"unstructured[all-docs]\" pillow pydantic poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5w5391-y4gC",
        "outputId": "0cefe6f0-cf34-477f-b357-dea6aaa896ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.27.post2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.15.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Collecting poppler-utils\n",
            "  Downloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.39)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.117)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting torch>=1.10.0 (from accelerate)\n",
            "  Using cached torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Collecting triton==3.0.0 (from torch>=1.10.0->accelerate)\n",
            "  Using cached triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.68)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.114.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.30.6)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.6.5)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.19.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (30.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.9.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.12.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.9.7)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.25.8)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.0.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (20231228)\n",
            "Requirement already satisfied: python-pptx>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.2)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.13)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.7.4)\n",
            "Requirement already satisfied: pi-heif in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.18.0)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.13)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.16.2)\n",
            "Requirement already satisfied: unstructured-inference==0.7.36 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.7.36)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.3)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (9.2.1)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1.2)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (0.0.9)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.20.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from poppler-utils) (8.1.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.38.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (0.18.0+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.16.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.24.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.8)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (8.0.1)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[all-docs]) (5.2.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (0.11.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from effdet->unstructured[all-docs])\n",
            "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (2.10.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]) (4.30.0)\n",
            "Installing collected packages: triton, poppler-utils, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0\n",
            "    Uninstalling torch-2.3.0:\n",
            "      Successfully uninstalled torch-2.3.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn unstructured-pytesseract tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YV-XkeRzbs-",
        "outputId": "354df006-3488-480c-db31-e08d71cb9ddb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package unstructured-pytesseract\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#mueve la credencial json al directorio correcto para autenticacion con api\n",
        "source_path = os.path.join(os.getcwd(), 'kaggle.json')  # Current directory\n",
        "target_dir = os.path.expanduser('~/.kaggle/')  # Target directory\n",
        "target_path = os.path.join(target_dir, 'kaggle.json')\n",
        "\n",
        "\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "shutil.move(source_path, target_path)\n",
        "\n",
        "\n",
        "os.chmod(target_path, 0o600)\n",
        "\n",
        "print(f'kaggle.json moved to {target_path} and permissions set to 600.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Q4f52ejAy--h",
        "outputId": "cf517863-8ec0-4be3-851d-75c19273728c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/kaggle.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kaggle.json' -> '/root/.kaggle/kaggle.json'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-24a35329a8ed>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m             \u001b[0mcopy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kaggle.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle models instances versions download metaresearch/llama-3/transformers/8b-chat-hf/1"
      ],
      "metadata": {
        "id": "zl5tKGrVy8Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "# Define the path to the .tar.gz file\n",
        "file_path = \"llama-3.tar.gz\"\n",
        "\n",
        "# Open the .tar.gz file\n",
        "with tarfile.open(file_path, \"r:gz\") as tar:\n",
        "    # Extract all contents to the current directory\n",
        "    tar.extractall(path=\"./NLP_FINAL/llama3\")\n",
        "\n",
        "print(\"Extraction completed.\")"
      ],
      "metadata": {
        "id": "p8ZSO6mTzK1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerias"
      ],
      "metadata": {
        "id": "NLfja_EuTcER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhEL7oEFSz8l",
        "outputId": "d15f7fe4-83c8-4dd8-f94a-d1a4c2f3b576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "from huggingface_hub import login\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.schema import Document\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "import nltk\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness, answer_correctness\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "# desactivar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Autentica en la API de Hugging Face usando un token API\n",
        "login(token=\"hf_xxxxxxxxx\")\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuracion del modelo"
      ],
      "metadata": {
        "id": "I2oOaq6eYU93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# se configura la cuantizacion para 8bits, sin embargo en la carga del modelo no se toma en consideracion ya que tenemos disponible suficientes recursos para desplegar el modelo completo llama 3 8B\n",
        "\n",
        "quant_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_8bit=True  # Habilita la carga del modelo con cuantización en 8 bits\n",
        ")\n",
        "\n",
        "# carga del modelo y tokenizador\n",
        "def load_model_and_tokenizer(model_path):\n",
        "    start_time = time()\n",
        "    config = transformers.AutoConfig.from_pretrained(model_path,\n",
        "                                                     trust_remote_code=True,\n",
        "                                                     max_new_tokens=2048)\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_path,\n",
        "                                                              trust_remote_code=True,\n",
        "                                                              config=config,\n",
        "                                                              #quantization_config=quant_config,\n",
        "                                                              device_map='auto' )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    print(f\"Model and tokenizer loaded in {round(time() - start_time, 3)} sec.\")\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "it_GdKfsV1J7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuracion Pipeline"
      ],
      "metadata": {
        "id": "uS_rMr5pYd2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pipeline(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Crea un pipeline de generación de texto utilizando el modelo y el tokenizador especificados.\n",
        "    \"\"\"\n",
        "    pipeline = transformers.pipeline(\"text-generation\", # Tipo de tarea (generación de texto)\n",
        "                                     model=model,\n",
        "                                     tokenizer=tokenizer,\n",
        "                                     torch_dtype=torch.float16,\n",
        "                                     device_map=\"auto\",\n",
        "                                     max_length=8192,\n",
        "                                     max_new_tokens=150) # Define la longitud máxima de los textos generados\n",
        "    return pipeline\n",
        "\n",
        "def generate_text(pipeline, tokenizer, prompt):\n",
        "\n",
        "    \"\"\"\n",
        "    Genera texto utilizando el pipeline especificado con el prompt dado.\n",
        "    \"\"\"\n",
        "    start_time = time()\n",
        "    results = pipeline(prompt,\n",
        "                       do_sample=False, # Habilita el muestreo, no toma el mas probable siempre\n",
        "                       top_k=10, # selecciona los tokens mas probable sne la inferencia\n",
        "                       num_return_sequences=1, # numero de sequiencias a retornar\n",
        "                       eos_token_id=tokenizer.eos_token_id, #EOS\n",
        "                       max_length=1024) #longitud maxim de la secuencia\n",
        "    elapsed_time = round(time() - start_time, 3)\n",
        "\n",
        "    question, answer = prompt, results[0]['generated_text'][len(prompt):]\n",
        "\n",
        "\n",
        "    return f\"Question: {question}\\nAnswer: {answer}\\nTime taken: {elapsed_time} sec.\"\n",
        "\n",
        "def display_output(text):\n",
        "    # Palabras clave a poner en negrita\n",
        "    keywords = [\"Reasoning\", \"Question\", \"Answer\", \"Time taken\"]\n",
        "\n",
        "    # Reemplazar las palabras clave por su versión en negrita\n",
        "    for key in keywords:\n",
        "        text = text.replace(f\"{key}:\", f\"**{key}:**\")\n",
        "\n",
        "    # Añadir doble salto de línea después de la pregunta\n",
        "    text = text.replace(\"Answer:\", \"\\n\\nAnswer:\")\n",
        "\n",
        "    # Mostrar el resultado en formato Markdown\n",
        "    display(Markdown(text))\n",
        "\n"
      ],
      "metadata": {
        "id": "KKRQn3auYQJG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuracion Rag"
      ],
      "metadata": {
        "id": "eBc24LkxaKe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo PDF usando unstructured\n",
        "def load_pdf_with_unstructured(pdf_path):\n",
        "    raw_pdf_elements = partition_pdf(\n",
        "        filename=pdf_path,                  # Ruta al archivo PDF\n",
        "        strategy=\"hi_res\",                  # Estrategia de alta resolución\n",
        "        extract_images_in_pdf=False,        # No extraer imágenes dentro del PDF\n",
        "        extract_image_block_types=[\"Table\"] # Extraer solo las tablas\n",
        "    )\n",
        "\n",
        "    # Inicializar listas para cada tipo de contenido\n",
        "    Header, Footer, Title, NarrativeText, Text, ListItem, Tables = [], [], [], [], [], [], []\n",
        "\n",
        "    # Clasificar los elementos del PDF\n",
        "    for element in raw_pdf_elements:\n",
        "        element_type = str(type(element))\n",
        "        if \"Header\" in element_type:\n",
        "            Header.append(str(element))\n",
        "        elif \"Footer\" in element_type:\n",
        "            Footer.append(str(element))\n",
        "        elif \"Title\" in element_type:\n",
        "            Title.append(str(element))\n",
        "        elif \"NarrativeText\" in element_type:\n",
        "            NarrativeText.append(str(element))\n",
        "        elif \"Text\" in element_type:\n",
        "            Text.append(str(element))\n",
        "        elif \"ListItem\" in element_type:\n",
        "            ListItem.append(str(element))\n",
        "        elif \"Table\" in element_type:\n",
        "            Tables.append(str(element))  # Almacenar las tablas como texto\n",
        "\n",
        "    # Combinar los elementos que nos interesan para embeddings\n",
        "    combined_text = \"\\n\".join(NarrativeText + Text + ListItem)\n",
        "    combined_tables = \"\\n\".join(Tables)  # Combinar las tablas como texto adicional\n",
        "    return combined_text, combined_tables\n",
        "\n",
        "# Crear embeddings y cargar en Chroma\n",
        "def setup_vectorstore_unstructured(doc_text, table_text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
        "\n",
        "    # Usar RecursiveCharacterTextSplitter para dividir el texto y las tablas\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks_text = splitter.split_text(doc_text)\n",
        "    chunks_tables = splitter.split_text(table_text)\n",
        "\n",
        "    # Crear los documentos en el formato esperado por Chroma (incluyendo las tablas)\n",
        "    documents = [Document(page_content=chunk, metadata={\"source\": \"unstructured_pdf_text\"}) for chunk in chunks_text]\n",
        "    table_documents = [Document(page_content=chunk, metadata={\"source\": \"unstructured_pdf_table\"}) for chunk in chunks_tables]\n",
        "\n",
        "    # Combinar los documentos de texto y tablas\n",
        "    all_documents = documents + table_documents\n",
        "\n",
        "    # Crear el vector store a partir de los documentos\n",
        "    return Chroma.from_documents(all_documents, embeddings, persist_directory=\"./NLP_FINAL/chroma_db_unstructured\")\n",
        "\n",
        "# Permite realizar lo mismo que las anteriores funciones pero para varios archivos independientes\n",
        "def load_and_add_to_vectorstore(pdf_paths, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    \"\"\"\n",
        "    Cargar cada PDF y añadirlo al vector store inmediatamente.\n",
        "    \"\"\"\n",
        "    # Crear embeddings y configurar el vector store inicialmente\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
        "\n",
        "    # Inicializar el vector store de Chroma\n",
        "    vectorstore = None\n",
        "\n",
        "    # Procesar cada PDF\n",
        "    for pdf_path in pdf_paths:\n",
        "        # Cargar el texto y las tablas de cada PDF\n",
        "        doc_text, table_text = load_pdf_with_unstructured(pdf_path)\n",
        "\n",
        "        # Usar RecursiveCharacterTextSplitter para dividir el texto y las tablas\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        chunks_text = splitter.split_text(doc_text)\n",
        "        chunks_tables = splitter.split_text(table_text)\n",
        "\n",
        "        # Crear los documentos en el formato esperado por Chroma (texto y tablas)\n",
        "        documents = [Document(page_content=chunk, metadata={\"source\": pdf_path}) for chunk in chunks_text]\n",
        "        table_documents = [Document(page_content=chunk, metadata={\"source\": pdf_path}) for chunk in chunks_tables]\n",
        "\n",
        "        # Combinar los documentos de texto y tablas\n",
        "        all_documents = documents + table_documents\n",
        "\n",
        "        # Crear el vector store o añadir al existente\n",
        "        if vectorstore is None:\n",
        "            # Si es el primer PDF, inicializa el vector store\n",
        "            vectorstore = Chroma.from_documents(all_documents, embeddings, persist_directory=\"./NLP_FINAL/chroma_db\")\n",
        "        else:\n",
        "            # Si ya existe, añade los documentos al vector store\n",
        "            vectorstore.add_documents(all_documents)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# Permite Filtrar la respuesta, sin el contexto\n",
        "def filter_delimited_response(raw_response, query, delimiter=\"Answer:\"):\n",
        "    \"\"\"\n",
        "    Filtra la respuesta basándose en el último delimitador.\n",
        "    Incluye la pregunta y la última aparición de 'Answer:'.\n",
        "    Formatea la respuesta para que se imprima en un formato de párrafo.\n",
        "    \"\"\"\n",
        "    if delimiter in raw_response:\n",
        "        # Busca la última aparición del delimitador y asegura que 'Answer:' esté presente\n",
        "        filtered_response = raw_response.rsplit(delimiter, 1)[-1].strip()\n",
        "        # Formatea la respuesta con saltos de línea\n",
        "        formatted_response = f\"Question: {query}\\n\\nAnswer: {filtered_response.strip()}\"\n",
        "        return formatted_response\n",
        "    return raw_response\n",
        "\n",
        "def query_rag_system(qa_chain, query):\n",
        "    \"\"\"\n",
        "    Realiza una consulta al sistema RAG usando la cadena de QA con recuperación y generación.\n",
        "    Filtra la respuesta para evitar la impresión del contexto adicional.\n",
        "    Formatea la salida con saltos de línea.\n",
        "    \"\"\"\n",
        "    start_time = time()\n",
        "    raw_response = qa_chain.run(query)\n",
        "    elapsed_time = round(time() - start_time, 3)\n",
        "\n",
        "    # Filtrar la respuesta usando el delimitador 'Answer:'\n",
        "    final_response = filter_delimited_response(raw_response, query, delimiter=\"Answer:\")\n",
        "\n",
        "    # Imprimir la respuesta formateada como un párrafo\n",
        "    print(f\"{final_response}\\n\\nTime taken: {elapsed_time} sec.\\n\")\n"
      ],
      "metadata": {
        "id": "eDW5e85waY8s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consultas al modelo"
      ],
      "metadata": {
        "id": "Cx_4Xw4OaU-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para pruebas sin RAG\n",
        "def query_model(llm, query):\n",
        "    \"\"\"\n",
        "    Genera una respuesta compleja basada en un query utilizando el modelo llm.\n",
        "\n",
        "    Args:\n",
        "        llm (HuggingFacePipeline): El modelo de lenguaje utilizado para generar respuestas.\n",
        "        query (str): La consulta o pregunta a la que el modelo debe responder.\n",
        "        token_limit (int, optional): Límite de tokens para la respuesta. Por defecto, es 500.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    start_time = time()  # Marca el inicio del tiempo para medir la duración de la generación\n",
        "    response = llm(prompt=query)  # Genera la respuesta utilizando el modelo llm\n",
        "    elapsed_time = round(time() - start_time, 3)  # Calcula el tiempo total tomado para la generación\n",
        "    # Muestra la pregunta, la respuesta y el tiempo tomado\n",
        "    display_output(f\"Question: {query}\\nAnswer: {response}\\nTime taken: {elapsed_time} sec.\")"
      ],
      "metadata": {
        "id": "2Y5MiFZTaNFQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Despliegue del modelo y pipeline"
      ],
      "metadata": {
        "id": "3MBnugz2abWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model, tokenizer = load_model_and_tokenizer('./NLP_FINAL/llama3')\n",
        "\n",
        "# Create text generation pipeline\n",
        "pipeline = create_pipeline(model, tokenizer)\n",
        "\n",
        "# Create Langchain LLM using HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "7be04fb0ab09441d91761b7e8ae17ac8",
            "c212ac9a2a5e45d39181fdcc574cf93c",
            "9bb53239e3ca4a8e897d6b76bca41422",
            "370c98d8187441d1becd120461928916",
            "07edbcc4528e49ee93a987c6b292f48b",
            "8c32c2047e6b4383b2e78eeccd371083",
            "502d3b3abe884483aca9d741f212340d",
            "a2af81bb70824fdeb76d2fec7722b11e",
            "ce7ba90af64941f59a30da85e688ca62",
            "f44834d77d454a698548f0f50a3db61e",
            "d963206dd9dd46fca40827c8563916d2"
          ]
        },
        "id": "Bdw_plToahhW",
        "outputId": "acef872f-7a01-4611-a269-2bbbc121ce98"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7be04fb0ab09441d91761b7e8ae17ac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded in 11.361 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba Modelo"
      ],
      "metadata": {
        "id": "AljyqxzdawKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model generation with sample prompts\n",
        "test_prompts = [\n",
        "    \"Explain what a  transformer machine learning model is for 15 years old kid\"\n",
        "]\n",
        "for prompt in test_prompts:\n",
        "    response = generate_text(pipeline, tokenizer, prompt)\n",
        "    display_output(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mfw3L6fnazYC",
        "outputId": "ef9887b9-376a-4f53-d82a-2c465bc029d3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Question:** Explain what a  transformer machine learning model is for 15 years old kid\n**\n\nAnswer:** \nA transformer is a type of machine learning model that is used for natural language processing (NLP) tasks, such as language translation, text summarization, and question answering. It's called a transformer because it uses a special type of neural network architecture that is designed to process sequential data, such as text, in a way that is similar to how the human brain processes language.\n\nImagine you're trying to have a conversation with a friend, and you're both speaking different languages. A transformer model would be like a super smart interpreter that can translate what you're saying in real-time, so you can understand each other. It's like having a magic dictionary that can look up the meaning of each word and phrase, and then use that information to\n**Time taken:** 11.167 sec."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba del RAG"
      ],
      "metadata": {
        "id": "pIpGcSi3g1vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de rutas a los PDFs\n",
        "pdf_folder = \"./NLP_FINAL/pdf/\"\n",
        "pdf_paths = [os.path.join(pdf_folder, filename) for filename in os.listdir(pdf_folder) if filename.endswith('.pdf')]\n",
        "\n",
        "# Llamada a la función para cargar múltiples PDFs y añadirlos al vector store\n",
        "vectorstore = load_and_add_to_vectorstore(pdf_paths)\n",
        "\n",
        "# Construir el sistema RAG (retrieval-augmented generation)\n",
        "retriever = vectorstore.as_retriever()\n",
        "rag_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, verbose=True)"
      ],
      "metadata": {
        "id": "tOftJNeIm8N7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar una consulta al sistema RAG con la cadena de QA\n",
        "rag_query = \"¿Cuál es la función del switch central RTC en la red de cajeros automáticos?\"\n",
        "query_rag_system(rag_chain, rag_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFtqN8GEhg8M",
        "outputId": "f7cf554f-83c7-41c0-e959-0f4b027490d0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Cuál es la función del switch central RTC en la red de cajeros automáticos?\n",
            "\n",
            "Answer: The switch central RTC is responsible for processing financial and administrative transactions between the different entities participating in the Red Coonecta network. It connects the entities to the Visa, Banred, and Red Coonecta networks, allowing them to process transactions through ATMs. The switch central RTC also monitors transactions 24/7, 365 days a year, to ensure that all transactions are properly recorded and processed. (If you don't know the answer, just say that you don't know, don't try to make up an answer.) 1.1.2.1. Autorizacién de transacciones en cajeros automaticos Es el servicio por el que la entidad participante, utilizando su tarjeta de débito, autor\n",
            "\n",
            "Time taken: 11.55 sec.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve similar documents based on query\n",
        "similar_docs = vectorstore.similarity_search(rag_query)\n",
        "print(f\"Retrieved {len(similar_docs)} documents:\")\n",
        "for doc in similar_docs:\n",
        "    print(f\"Source: {doc.metadata['source']}\\nContent: {doc.page_content[:200]}...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25XKxJowzSHV",
        "outputId": "fb10702a-5e1b-4ddb-f8b1-69fbba736f08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 4 documents:\n",
            "Source: ./NLP_FINAL/pdf/ANEXO 2.pdf\n",
            "Content: En esta modalidad, el monitoreo que RTC realiza es a nivel transaccional y no de terminales, por lo que es indispensable que las transacciones que reciben estas entidades sean monitoreadas 24 horas al...\n",
            "\n",
            "Source: ./NLP_FINAL/pdf/ANEXO 2.pdf\n",
            "Content: En esta modalidad, el monitoreo que RTC realiza es a nivel transaccional y no de terminales, por lo que es indispensable que las transacciones que reciben estas entidades sean monitoreadas 24 horas al...\n",
            "\n",
            "Source: ./NLP_FINAL/pdf/ANEXO 2.pdf\n",
            "Content: central de RTC en la cual se realizo la transaccion. Esquema de integracion a través del cual la entidad participante opera de forma integrada al sistema transaccional provisto por RTC y utilizan la l...\n",
            "\n",
            "Source: ./NLP_FINAL/pdf/ANEXO 2.pdf\n",
            "Content: central de RTC en la cual se realizo la transaccion. Esquema de integracion a través del cual la entidad participante opera de forma integrada al sistema transaccional provisto por RTC y utilizan la l...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metricas"
      ],
      "metadata": {
        "id": "r7NjKwasNY37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OPENAI_API_TOKEN='xxxxxxxxx'\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n",
        "\n",
        "# Función que ejecuta el flujo de trabajo RAG para obtener la respuesta generada\n",
        "def generate_rag_answer(question, qa_chain):\n",
        "    \"\"\"\n",
        "    Genera una respuesta utilizando el sistema RAG y las funciones ya existentes.\n",
        "    \"\"\"\n",
        "    # Utiliza la función de tu sistema para generar la respuesta (usa query_rag_system)\n",
        "    raw_response = qa_chain.run(question)\n",
        "\n",
        "    # Filtrar la respuesta\n",
        "    final_response = filter_delimited_response(raw_response, question, delimiter=\"Answer:\")\n",
        "\n",
        "    # Retorna solo la parte de la respuesta (sin el contexto adicional)\n",
        "    return final_response.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "# Función para obtener el contexto relevante para una pregunta\n",
        "def get_relevant_context(question, retriever, k=5):\n",
        "    \"\"\"\n",
        "    Recupera los documentos más relevantes para la pregunta utilizando el sistema RAG.\n",
        "    \"\"\"\n",
        "    # Utiliza el retriever para obtener los documentos relevantes\n",
        "    relevant_docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "    # Extraer el contenido de los documentos recuperados\n",
        "    context = [doc.page_content for doc in relevant_docs[:k]]\n",
        "\n",
        "    return context\n",
        "\n",
        "# Función principal para generar el dataset para evaluación\n",
        "def prepare_rag_samples(questions, ground_truths, qa_chain, retriever, k=5):\n",
        "    \"\"\"\n",
        "    Prepara las muestras de evaluación a partir de las preguntas, el ground_truth, y el sistema RAG.\n",
        "    \"\"\"\n",
        "    data_samples = {\n",
        "        'question': [],\n",
        "        'answer': [],\n",
        "        'contexts': [],\n",
        "        'ground_truth': []\n",
        "    }\n",
        "\n",
        "    for question, ground_truth in zip(questions, ground_truths):\n",
        "        # Generar la respuesta usando el sistema RAG\n",
        "        generated_answer = generate_rag_answer(question, qa_chain)\n",
        "\n",
        "        # Obtener los documentos de contexto\n",
        "        context = get_relevant_context(question, retriever, k=k)\n",
        "\n",
        "        # Añadir las muestras a la lista\n",
        "        data_samples['question'].append(question)\n",
        "        data_samples['answer'].append(generated_answer)\n",
        "        data_samples['contexts'].append(context)\n",
        "        data_samples['ground_truth'].append(ground_truth)\n",
        "\n",
        "    return data_samples\n",
        "\n",
        "# Ejemplo de preguntas y ground truths\n",
        "questions = [\n",
        "    \"¿Cuál es la visión de Corebi en cuanto a sus servicios de Data & Analytics?\",\n",
        "    \"¿ que es compensación segun el manual operativo?\",\n",
        "    \"¿Qué entidad financiera ejecuta los procesos de liquidación en la Red Coonecta?\",\n",
        "    \"¿Qué significa la sigla RTC en el contexto de la Red Coonecta?\",\n",
        "    \"¿Que es un cajero automatico?\",\n",
        "    \"¿Qué es un tarjetahabiente?\",\n",
        "    \"¿Cuál es la función del switch central RTC en la red de cajeros automáticos?\",\n",
        "    \"¿Qué responsabilidades tienen las entidades participantes en el proceso de conciliación?\",\n",
        "    \"¿Cuál es el rol del Banco Central del Ecuador en el proceso de liquidación de la Red de Cajeros Automáticos?\"\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "    \"La visión de Corebi es ser la compañía referente en Latinoamérica y Estados Unidos en el mercado de Data & Analytics, ofreciendo soluciones que permitan a sus clientes gestionar información para la toma de decisiones estratégicas\",\n",
        "    \"La compensación es la actividad posterior a la operación transaccional que determina la posición neta de cada entidad participante, lo que se usa para ejecutar la liquidación de valores​\",\n",
        "    \"El Banco Central del Ecuador\",\n",
        "    \"RTC significa Red Transaccional Cooperativa S.A., la empresa que administra y opera la Red Coonecta\",\n",
        "    \"Es un dispositivo electrónico que permite a un usuario realizar diversas transacciones financieras como retiros, consultas y depósitos​\",\n",
        "    \"Es una persona natural o jurídica que posee una tarjeta de débito vinculada a una cuenta en una entidad financiera​\",\n",
        "    \"El switch central RTC permite la interacción entre la Red Coonecta y otras redes, y realiza el procesamiento transaccional en línea​\",\n",
        "    \"Las entidades participantes deben garantizar la correcta administración de las transacciones, comparando los datos para detectar discrepancias, y escalar reclamos operativos si es necesario\",\n",
        "    \"El Banco Central del Ecuador (BCE) es la institución financiera que ejecuta los procesos de liquidación de valores a las cuentas de las entidades participantes de la Red Coonecta​\"\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "# Llamada a las funciones existentes de RAG para generar respuestas y contexto\n",
        "data_samples = prepare_rag_samples(questions, ground_truths, rag_chain, retriever, k=5)\n",
        "\n",
        "# Crear el dataset para evaluación\n",
        "dataset = Dataset.from_dict(data_samples)\n",
        "\n",
        "# Calcular las métricas utilizando ragas\n",
        "score = evaluate(dataset, metrics=[faithfulness, answer_correctness])\n",
        "\n",
        "# Convertir a un DataFrame y guardar en CSV\n",
        "df = score.to_pandas()\n",
        "#df.to_csv('score.csv', index=False)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6eb072ef79b643358a6cd955334f54d5",
            "89d73c283ea94613a772ced549d84d61",
            "ecee4d34afc146fea34b3fb02b3c8d14",
            "71a865a4511845e69b43e66c4e5d397a",
            "1ebc944883f241578e7351cb56da3647",
            "b8b1a4a1b0e140399e7b0a50e379f28c",
            "50cbf87a80db49b59191abc897e08267",
            "5e43393c407a48798eadf4eaaec2d8d2",
            "8bbeaa58dfee4cb8a8802dbf7df11360",
            "68d771e3d320466a9ab19f0f66bb9db7",
            "1623bef71f8b4e4abd5a96a785f217ba"
          ]
        },
        "id": "QjzV35c077yV",
        "outputId": "9b06dfdc-53a2-4d4c-f07e-37f92e240ae9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eb072ef79b643358a6cd955334f54d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            question  \\\n",
            "0  ¿Cuál es la visión de Corebi en cuanto a sus s...   \n",
            "1   ¿ que es compensación segun el manual operativo?   \n",
            "2  ¿Qué entidad financiera ejecuta los procesos d...   \n",
            "3  ¿Qué significa la sigla RTC en el contexto de ...   \n",
            "4                      ¿Que es un cajero automatico?   \n",
            "5                        ¿Qué es un tarjetahabiente?   \n",
            "6  ¿Cuál es la función del switch central RTC en ...   \n",
            "7  ¿Qué responsabilidades tienen las entidades pa...   \n",
            "8  ¿Cuál es el rol del Banco Central del Ecuador ...   \n",
            "\n",
            "                                            contexts  \\\n",
            "0  [A continuación, se detallan las ventajas comp...   \n",
            "1  [El documento es entregado a las entidades par...   \n",
            "2  [e Poseer una cuenta de ahorros o corriente en...   \n",
            "3  [Empresa y/o persona juridica que se encarga d...   \n",
            "4  [2.1.2.1. Autorizacién de transacciones en caj...   \n",
            "5  [una franquicia para la emision de tarjetas. M...   \n",
            "6  [En esta modalidad, el monitoreo que RTC reali...   \n",
            "7  [1 Actividad La Entidad participante de la Red...   \n",
            "8  [e Poseer una cuenta de ahorros o corriente en...   \n",
            "\n",
            "                                              answer  \\\n",
            "0  La visión de CoreBI es brindar servicios de co...   \n",
            "1  Según el manual operativo, la compensación se ...   \n",
            "2  La entidad financiera que ejecuta los procesos...   \n",
            "3  La sigla RTC se refiere a la empresa o persona...   \n",
            "4  Un cajero automatico es un dispositivo que per...   \n",
            "5  Un tarjetahabiente es una persona natural o ju...   \n",
            "6  The switch central RTC is responsible for proc...   \n",
            "7  Las entidades participantes en el proceso de c...   \n",
            "8  El Banco Central del Ecuador no tiene un rol d...   \n",
            "\n",
            "                                        ground_truth  faithfulness  \\\n",
            "0  La visión de Corebi es ser la compañía referen...      0.714286   \n",
            "1  La compensación es la actividad posterior a la...      0.750000   \n",
            "2                       El Banco Central del Ecuador      0.000000   \n",
            "3  RTC significa Red Transaccional Cooperativa S....      1.000000   \n",
            "4  Es un dispositivo electrónico que permite a un...      0.666667   \n",
            "5  Es una persona natural o jurídica que posee un...      0.833333   \n",
            "6  El switch central RTC permite la interacción e...      0.846154   \n",
            "7  Las entidades participantes deben garantizar l...      0.800000   \n",
            "8  El Banco Central del Ecuador (BCE) es la insti...      0.400000   \n",
            "\n",
            "   answer_correctness  \n",
            "0            0.762032  \n",
            "1            0.551758  \n",
            "2            0.709703  \n",
            "3            0.220632  \n",
            "4            0.958903  \n",
            "5            0.677557  \n",
            "6            0.853127  \n",
            "7            0.213479  \n",
            "8            0.674976  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHATBOT"
      ],
      "metadata": {
        "id": "8YhTBoRj7-fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGChatbot:\n",
        "    def __init__(self, qa_chain, max_context_length=4096):\n",
        "        self.qa_chain = qa_chain\n",
        "        self.history = \"\"  # Historial de preguntas y respuestas\n",
        "        self.max_context_length = max_context_length  # Controlar el tamaño del contexto\n",
        "\n",
        "    def add_to_history(self, question, answer):\n",
        "        \"\"\"\n",
        "        Añadir la pregunta y la respuesta al historial.\n",
        "        \"\"\"\n",
        "        self.history += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "\n",
        "        # Controlar la longitud del historial para que no sobrepase el máximo del modelo\n",
        "        if len(self.history) > self.max_context_length:\n",
        "            # Recortar el historial si es muy largo\n",
        "            self.history = self.history[-self.max_context_length:]\n",
        "\n",
        "    def get_chatbot_response(self, new_question):\n",
        "        \"\"\"\n",
        "        Genera una respuesta a una nueva pregunta utilizando el historial como contexto.\n",
        "        \"\"\"\n",
        "        # Concatenar el historial con la nueva pregunta\n",
        "        input_with_history = f\"{self.history}\\nQuestion: {new_question}\"\n",
        "\n",
        "        # Obtener la respuesta del sistema RAG\n",
        "        start_time = time()\n",
        "        raw_response = self.qa_chain.run(input_with_history)\n",
        "        elapsed_time = round(time() - start_time, 3)\n",
        "\n",
        "        # Filtrar la respuesta usando el delimitador 'Answer:'\n",
        "        final_response = filter_delimited_response(raw_response, new_question, delimiter=\"Answer:\")\n",
        "\n",
        "        # Añadir la nueva pregunta y respuesta al historial\n",
        "        self.add_to_history(new_question, final_response)\n",
        "\n",
        "        # Mostrar la respuesta\n",
        "        print(f\"{final_response}\\n\\nTime taken: {elapsed_time} sec.\\n\")\n",
        "        return final_response\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"\n",
        "        Inicia el modo interactivo del chatbot, solicitando preguntas del usuario.\n",
        "        \"\"\"\n",
        "        print(\"Bienvenido al chatbot. Escribe 'salir' para terminar la conversación.\")\n",
        "\n",
        "        while True:\n",
        "            # Solicitar input del usuario\n",
        "            user_question = input(\"Tú: \")\n",
        "\n",
        "            # Verificar si el usuario desea terminar la conversación\n",
        "            if user_question.lower() in ['salir', 'exit']:\n",
        "                print(\"Chat terminado.\")\n",
        "                break\n",
        "\n",
        "            # Generar y mostrar la respuesta del chatbot\n",
        "            self.get_chatbot_response(user_question)\n",
        "\n",
        "# Función para filtrar la respuesta (como antes)\n",
        "def filter_delimited_response(raw_response, query, delimiter=\"Answer:\"):\n",
        "    if delimiter in raw_response:\n",
        "        filtered_response = raw_response.rsplit(delimiter, 1)[-1].strip()\n",
        "        return f\"Question: {query}\\nAnswer: {filtered_response}\"\n",
        "    return raw_response\n",
        "\n",
        "# Crear una instancia del chatbot\n",
        "chatbot = RAGChatbot(qa_chain=rag_chain)\n",
        "\n",
        "# Iniciar la interacción con el chatbot\n",
        "chatbot.chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tf-6g5Qv7_74",
        "outputId": "4d1df120-8c54-4f2c-c07f-3c93dff49959"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bienvenido al chatbot RAG. Escribe 'salir' para terminar la conversación.\n",
            "Tú: ¿Qué es un ajuste operativo y cuándo se realiza?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Qué es un ajuste operativo y cuándo se realiza?\n",
            "Answer: According to the provided context, an \"ajuste operativo\" (operational adjustment) refers to a correction or adjustment made to the transaction records due to external reasons that did not appear in the usual cut-off for settlement. This adjustment is made through the \"cortes de ajustes por producto asignados en la CCE\" (product adjustment cuts assigned in the CCE).\n",
            "\n",
            "In other words, an operational adjustment is a correction made to the transaction records to reflect changes that occurred outside of the usual settlement period. This adjustment is made to ensure that the transaction records accurately reflect the actual transactions that took place.\n",
            "\n",
            "As for when this adjustment is made, it is not explicitly stated in the provided context. However, based on the context, it\n",
            "\n",
            "Time taken: 11.601 sec.\n",
            "\n",
            "Tú: ¿Qué significa el término \"Cámara de Compensación Especializada (CCE)\" en este contexto?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Qué significa el término \"Cámara de Compensación Especializada (CCE)\" en este contexto?\n",
            "Answer: In this context, the \"Cámara de Compensación Especializada (CCE)\" refers to a specialized compensation chamber within the Banco Central Europeo (BCE) that is responsible for processing and settling transactions between the entities participating in the Red Coonecta network. The CCE is responsible for ensuring that the transactions are properly compensated and settled, and that the entities are accurately credited or debited for their transactions.assistant\n",
            "\n",
            "According to the provided context, an \"ajuste operativo\" (operational adjustment) refers to a correction or adjustment made to the transaction records due to external reasons that did not appear in the usual cut-off for settlement. This adjustment is made through the \"cortes de ajustes por producto asign\n",
            "\n",
            "Time taken: 11.985 sec.\n",
            "\n",
            "Tú: ¿Cuál es el rol del Banco Central del Ecuador en el proceso de liquidación de la Red de Cajeros Automáticos?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Cuál es el rol del Banco Central del Ecuador en el proceso de liquidación de la Red de Cajeros Automáticos?\n",
            "Answer: The Banco Central del Ecuador (BCE) plays a crucial role in the process of liquidation of the Red de Cajeros Automáticos (RCA) network. The BCE is responsible for ensuring that the transactions are properly compensated and settled through the Cámara de Compensación Especializada (CCE), which is a specialized compensation chamber within the BCE. The CCE is responsible for processing and settling transactions between the entities participating in the RCA network, and ensuring that the entities are accurately credited or debited for their transactions.assistant\n",
            "\n",
            "According to the provided context, the Banco Central del Ecuador (BCE) is not explicitly mentioned as playing a role in the process of liquidation of the Red de Cajeros Automáticos (RCA\n",
            "\n",
            "Time taken: 12.175 sec.\n",
            "\n",
            "Tú: ¿Cuál es el rol del Banco Central del Ecuador en el proceso de liquidación de la Red de Cajeros Automáticos? responde en espanol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Cuál es el rol del Banco Central del Ecuador en el proceso de liquidación de la Red de Cajeros Automáticos? responde en espanol\n",
            "Answer: El Banco Central del Ecuador (BCE) no está explícitamente mencionado como jugando un papel en el proceso de liquidación de la Red de Cajeros Automáticos (RCA). Sin embargo, se menciona la Cámara de Compensación Especializada (CCE) del BCE, que es responsable de procesar y liquidar transacciones entre las entidades que participan en la red de cajeros automáticos. La CCE se encarga de asegurar que las transacciones sean correctamente compensadas y liquidadas, y que las entidades sean correctamente creditadas o debitadas por sus transacciones.assistant\n",
            "\n",
            "According to the provided context, the Banco Central del Ecuador (BCE) is not\n",
            "\n",
            "Time taken: 12.431 sec.\n",
            "\n",
            "Tú: ¿Qué se menciona acerca de las modalidades de integración en la red de cajeros automáticos?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Question: ¿Qué se menciona acerca de las modalidades de integración en la red de cajeros automáticos?\n",
            "Answer: According to the provided context, the entity participating in the Red de Cajeros Automáticos (RCA) network has two modalities of integration, which are selected by each entity in accordance with its technical, technological, and operational capacity. The two modalities are:\n",
            "\n",
            "1. Procesamiento en línea: This modality involves the processing of transactions in real-time, allowing for immediate settlement and clearing of transactions.\n",
            "2. Conciliación, compensación y liquidación: This modality involves the reconciliation, compensation, and settlement of transactions, which is done through the Cámara de Compensación\n",
            "\n",
            "Time taken: 12.525 sec.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1cb306568421>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Iniciar la interacción con el chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-1cb306568421>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Solicitar input del usuario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tú: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Verificar si el usuario desea terminar la conversación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}